"""
AI Chat View - ë‹¨ìˆœí™”ëœ ë²„ì „
"""

import streamlit as st
import time
from src.rag.retriever import ContextRetriever
from src.llm.client import LLMClient


def render_chat_view(data):
    st.header("ğŸ¤– AI Academic Assistant")
    st.caption("í•™ì‚¬ ì •ë³´ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ì§ˆë¬¸í•˜ì„¸ìš”.")

    # ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
    if "chat_history" not in st.session_state:
        st.session_state["chat_history"] = [
            {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ì´ë²ˆ ì£¼ ê³¼ì œë‚˜ ê³µì§€ì‚¬í•­ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”."}
        ]

    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
    for msg in st.session_state["chat_history"]:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"], unsafe_allow_html=True)

    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("Ex: 'ì´ë²ˆ ì£¼ ë§ˆê° ê³¼ì œ ì•Œë ¤ì¤˜'"):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state["chat_history"].append({"role": "user", "content": prompt})
        
        with st.chat_message("user"):
            st.markdown(prompt)

        # AI ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            
            with st.spinner("ìƒê° ì¤‘..."):
                try:
                    # í˜„ì¬ í•™ê¸°
                    current_semester = st.session_state.get("current_semester")
                    
                    # ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
                    retriever = ContextRetriever(data, semester=current_semester)
                    relevant_items = retriever.retrieve_context(mode="query", query=prompt)
                    
                    st.caption(f"â„¹ï¸ ê²€ìƒ‰ ê²°ê³¼: {len(relevant_items)}ê°œ í•­ëª©")
                    
                    # LLM í˜¸ì¶œ
                    response_text = _generate_response(prompt, relevant_items)
                    
                    # ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼
                    full_response = ""
                    for i in range(0, len(response_text), 5):
                        full_response = response_text[:i+5]
                        time.sleep(0.01)
                        message_placeholder.markdown(full_response + "â–Œ")
                    
                    message_placeholder.markdown(full_response)
                    
                    # ê¸°ë¡ì— ì¶”ê°€
                    st.session_state["chat_history"].append({
                        "role": "assistant", 
                        "content": full_response
                    })
                    
                    # ì°¸ê³  ìë£Œ í‘œì‹œ
                    if relevant_items:
                        with st.expander(f"ğŸ“š ì°¸ê³  ìë£Œ ({len(relevant_items)}ê°œ)", expanded=False):
                            for item in relevant_items[:5]:
                                st.markdown(f"- **{item.get('title', 'No Title')}** ({item.get('course_name', '')})")
                    
                except Exception as e:
                    error_msg = f"ì˜¤ë¥˜ ë°œìƒ: {e}"
                    message_placeholder.error(error_msg)
                    st.session_state["chat_history"].append({
                        "role": "assistant",
                        "content": error_msg
                    })


def _generate_response(query: str, context_items: list) -> str:
    """LLMìœ¼ë¡œ ì‘ë‹µ ìƒì„±"""
    import httpx
    from datetime import datetime
    
    # ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
    context_str = ""
    for item in context_items[:10]:
        title = item.get("title", "")
        course = item.get("course_name", "")
        content = str(item.get("content_clean", "") or item.get("body_text", ""))[:500]
        due = item.get("due_date", "")
        context_str += f"- [{course}] {title} (Due: {due})\n  {content[:200]}...\n"
    
    if not context_str:
        context_str = "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    today_str = datetime.now().strftime("%Y-%m-%d %A")
    sys_prompt = f"""You are a helpful academic assistant. Today is {today_str}.
Answer the user's question based on the following course data.
Use Korean and be concise. If you don't know, say so."""

    user_msg = f"Context:\n{context_str}\n\nQuestion: {query}"
    
    try:
        client = LLMClient()
        payload = {
            "model": client.model,
            "messages": [
                {"role": "system", "content": sys_prompt},
                {"role": "user", "content": user_msg}
            ],
            "stream": False,
            "options": {"temperature": 0.3}
        }
        
        with httpx.Client(timeout=60) as c:
            resp = c.post(f"{client.api_url}/api/chat", json=payload)
            resp.raise_for_status()
            return resp.json().get("message", {}).get("content", "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    except Exception as e:
        return f"LLM ì—°ê²° ì˜¤ë¥˜: {e}"
