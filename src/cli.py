import argparse
import logging
import asyncio
import sys

if sys.platform == 'win32':
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
from pathlib import Path

from src.config.settings import Settings
from src.domains.canvas import CanvasCrawler
from src.domains.learningx import download_learningx_files
from src.domains.notices import NoticesCrawler, load_board_configs
from src.records.writer import RecordWriter
from src.app import collect_cookies


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="í•™êµ/ìº”ë²„ìŠ¤ í¬ë¡¤ëŸ¬")
    sub = parser.add_subparsers(dest="target", required=True)

    canvas = sub.add_parser("canvas", help="ìº”ë²„ìŠ¤ ì „ì²´/íŠ¹ì • ê³¼ëª© í¬ë¡¤ë§")
    canvas.add_argument(
        "--course-id",
        action="append",
        help="íŠ¹ì • ì½”ìŠ¤ IDë§Œ í¬ë¡¤ë§(ì—¬ëŸ¬ ë²ˆ ì§€ì • ê°€ëŠ¥). ì—†ìœ¼ë©´ í™œì„± ê³¼ëª© ì „ì²´.",
    )
    canvas.add_argument(
        "--download-files",
        action="store_true",
        help="ìº”ë²„ìŠ¤ íŒŒì¼ ë©”íƒ€ ì™¸ ì‹¤ì œ íŒŒì¼ë„ ë‹¤ìš´ë¡œë“œ",
    )

    notices = sub.add_parser("notices", help="í•™êµ/í•™ê³¼ ê³µì§€ í¬ë¡¤ë§")
    notices.add_argument(
        "--config",
        type=Path,
        default=Path("boards.dankook.json"),
        help="ê²Œì‹œíŒ ì„¤ì • JSON ê²½ë¡œ (ê¸°ë³¸: boards.dankook.json)",
    )
    notices.add_argument(
        "--max-pages",
        type=int,
        default=1,
        help="ê²Œì‹œíŒë³„ ìµœëŒ€ í˜ì´ì§€ ìˆ˜ì§‘ ë²”ìœ„(ê¸°ë³¸ 1)",
    )

    summarize = sub.add_parser("summarize", help="ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ìš”ì•½í•˜ì—¬ ë¦¬í¬íŠ¸ ìƒì„±")
    summarize.add_argument(
        "--days",
        type=int,
        default=7,
        help="ìµœê·¼ Nì¼ ë°ì´í„°ë§Œ ìš”ì•½ (NotImplemented)",
    )

    return parser


def main() -> None:
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s",
    )
    parser = build_parser()
    args = parser.parse_args()

    settings = Settings.from_env()
    writer = RecordWriter(base_path=settings.raw_records_dir)

    if args.target == "canvas":
        # íŒŒì¼ ë‹¤ìš´ë¡œë“œ ëª¨ë“œì¼ ê²½ìš°, ë¨¼ì € ë¸Œë¼ìš°ì €ë¥¼ ë„ì›Œ ë¡œê·¸ì¸ ì„¸ì…˜ì„ ê°±ì‹ í•œë‹¤.
        if args.download_files:
            print(">>> [ì•ˆë‚´] íŒŒì¼ ë‹¤ìš´ë¡œë“œë¥¼ ìœ„í•´ ë¸Œë¼ìš°ì € ë¡œê·¸ì¸ì„ ì§„í–‰í•©ë‹ˆë‹¤.")
            asyncio.run(
                collect_cookies(
                    name="ìº”ë²„ìŠ¤",
                    url=settings.canvas_base_url or "https://canvas.dankook.ac.kr",
                    out_path=Path("data/cookies_canvas.json"),
                    user_data_dir=None,  # ìë™ ë¡œê·¸ì¸ ì‚¬ìš©í•˜ë¯€ë¡œ ì˜êµ¬ í”„ë¡œí•„ ë¶ˆí•„ìš” & ì¶©ëŒ ë°©ì§€
                )
            )

        crawler = CanvasCrawler(
            settings=settings,
            writer=writer,
            download_files=args.download_files,
        )
        courses = crawler.crawl(course_ids=args.course_id)
        
        if args.download_files and courses:
            # learningx íŒŒì¼ ë‹¤ìš´ë¡œë“œ (Playwright ì‚¬ìš©)
            course_ids = [c["id"] for c in courses]
            
            from src.domains.downloader import download_canvas_files
            
            asyncio.run(
                download_learningx_files(
                    base_url=settings.canvas_base_url or "https://canvas.dankook.ac.kr",
                    course_ids=course_ids,
                    cookies_path=Path("data/cookies_canvas.json"),
                    files_dir=settings.files_dir,
                    raw_dir=settings.raw_records_dir,
                    user_data_dir=None,
                )
            )
            
            # ì¼ë°˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¶”ê°€
            asyncio.run(
                download_canvas_files(
                    base_url=settings.canvas_base_url or "https://canvas.dankook.ac.kr",
                    course_ids=course_ids,
                    cookies_path=Path("data/cookies_canvas.json"),
                    files_dir=settings.files_dir,
                    user_data_dir=None,
                    raw_dir=settings.raw_records_dir,
                )
            )
            
    elif args.target == "notices":
        boards = load_board_configs(args.config, settings.notices_base_url)
        crawler = NoticesCrawler(settings=settings, writer=writer)
        crawler.crawl(boards=boards, max_pages=args.max_pages)

    elif args.target == "summarize":
        from src.processing.metadata import MetadataExtractor
        from src.llm.client import LLMClient
        import json
        import os
        from collections import defaultdict

        print(">>> [ìš”ì•½] ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤ (Local Ollama)...")
        
        # 1. ë ˆì½”ë“œ ë¡œë“œ
        records = []
        records_path = settings.raw_records_dir / "records.jsonl"
        if records_path.exists():
            with open(records_path, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        records.append(json.loads(line))
                    except:
                        pass
        
        extractor = MetadataExtractor()
        
        # ë°ì´í„° ê·¸ë£¹í™” (Course ID ê¸°ì¤€)
        courses_data = defaultdict(list)
        course_names = {}

        # 2. ë ˆì½”ë“œ ì²˜ë¦¬ ë° ë¶„ë¥˜
        for rec in records:
            # ì½”ìŠ¤ ì •ë³´ ì¶”ì¶œ (Record íƒœê·¸ë‚˜ í˜ì´ë¡œë“œ í™œìš©)
            cid = rec.get("payload", {}).get("course_id")
            if not cid:
                # íƒœê·¸ì—ì„œ ì¶”ë¡  (canvas, COURSE_CODE)
                tags = rec.get("tags", [])
                if len(tags) >= 2 and tags[0] == "canvas":
                    # tags[1]ì´ ë³´í†µ ì½”ìŠ¤ ì½”ë“œ. IDëŠ” ì•„ë‹˜. í•˜ì§€ë§Œ ê·¸ë£¹í•‘ í‚¤ë¡œ ì‚¬ìš© ê°€ëŠ¥.
                    cid = tags[1]
            
            if not cid:
                cid = "common" # ê³µí†µ/ê¸°íƒ€

            # ì½”ìŠ¤ ì´ë¦„ ì €ì¥ (Categoryê°€ courseì¸ ê²½ìš°)
            if rec.get("category") == "course":
                course_names[str(rec.get("payload", {}).get("id"))] = rec.get("title")
                course_names[cid] = rec.get("title") # ì½”ë“œ ë§¤í•‘ ì‹œë„

            meta = extractor.summarize_record(rec)
            if meta["title"] and meta["title"] != "No Title":
                courses_data[cid].append(meta)

        # 3. íŒŒì¼ ì²˜ë¦¬
        files_root = settings.files_dir
        if files_root.exists():
            # files_dir êµ¬ì¡°: data/files/{course_id}/...
            for course_dir in files_root.iterdir():
                if course_dir.is_dir():
                    cid = course_dir.name
                    for fpath in course_dir.rglob("*"):
                        if fpath.is_file() and fpath.suffix.lower() in [".pdf", ".pptx", ".docx"]:
                            text = extractor.extract_text_from_file(fpath)
                            if text:
                                courses_data[cid].append({
                                    "category": "file",
                                    "title": fpath.name,
                                    "content_summary": text[:500], # ë¡œì»¬ ëª¨ë¸ í† í° ì ˆì•½
                                    "path": str(fpath),
                                    "date": "File Found"
                                })

        # 4. LLM í˜¸ì¶œ (ê³¼ëª©ë³„ ìˆœì°¨ ì‹¤í–‰)
        client = LLMClient(model="gpt-oss") # ì‚¬ìš©ì ì§€ì • ëª¨ë¸
        full_report = "# ğŸ“ í•™ì‚¬ ìš”ì•½ ë¦¬í¬íŠ¸ (by Ollama)\n\n"
        
        sorted_courses = sorted(courses_data.keys())
        total_courses = len(sorted_courses)
        
        for idx, cid in enumerate(sorted_courses, 1):
            items = courses_data[cid]
            if not items:
                continue
                
            c_name = course_names.get(cid, cid)
            if c_name == "common": c_name = "ğŸ“¢ ì¼ë°˜ ê³µì§€ / ê¸°íƒ€"
            
            print(f"[{idx}/{total_courses}] '{c_name}' ìš”ì•½ ìƒì„± ì¤‘... ({len(items)} í•­ëª©)")
            
            result = client.generate_course_report(str(c_name), items)
            
            # JSON ê²°ê³¼ì—ì„œ ìš”ì•½ë³¸ë§Œ ì¶”ì¶œí•˜ì—¬ ì¶œë ¥
            summary_text = result.get("summary", "ìš”ì•½ ì—†ìŒ")
            dt_stats = f"Deadlines: {len(result.get('deadlines', []))}, Notices: {len(result.get('announcements', []))}"
            
            full_report += f"## {c_name}\n\n{summary_text}\n\n*({dt_stats})*\n\n---\n\n"

        out_file = "report.md"
        with open(out_file, "w", encoding="utf-8") as f:
            f.write(full_report)
        
        print(f"\nâœ… ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {out_file}")

    else:
        parser.error("ì•Œ ìˆ˜ ì—†ëŠ” ëŒ€ìƒì…ë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
